# Review of Prior Research Notes (21JUN25) 

## Key Ideas and Hypotheses 

 Meaning in Information Theory: A central idea is the need to incorporate semantic meaning into classical information theory. The notes highlight that Shannon’s framework, while powerful, lacks a notion of meaning or significance. The author hypothesizes that extending information theory to account for what information means (not just how much of it there is) could be crucial for applying it to complex domains like biology or cognition. In essence, how can meaning be formally introduced into information measures? This question underpins much of the inquiry. 

 Cognition as Computation: Another key hypothesis is examining whether cognition (thought processes, intelligence) can be fully understood as a form of computation. The notes reference literature suggesting that since cognition processes natural information , it can be viewed as computation “in the generic sense”. However, the author is questioning whether the mind is equivalent to any specific kind of computation (like a digital computer analogy) or if cognition involves a different computational paradigm. This reflects a hypothesis that while the brain computes in some broad sense, it may not neatly fit existing computational models – pointing to a potential gap in the computational theory of mind. 

 Defining “Meaning”: The research notes also delve into the concept of meaning itself. For example, the author brings in a linguistic/philosophical angle by mentioning the Russian word “ смысл ” (meaning “sense/meaning”) in the context of introducing meaning. This suggests a hypothesis that understanding how different languages and fields define “meaning” could help in formulating a more rigorous definition for use in computational systems. In other words, the groundwork includes clarifying what we mean by "meaning," so that it can be used in scientific hypotheses. This is likely tied to the broader goal of bridging human-like understanding with information processing in machines. 

## Notable Findings and Partial Conclusions 

 Generic vs. Specific Computation: From the notes, one notable conclusion drawn (via the literature) is that cognition does involve computation, but not necessarily of a known specific type. A cited source observes that because cognition involves processing information from the environment, it “amounts to computation in the generic sense”. However , it also explicitly states that it does not follow that cognition is any particular kind of computation (such as strictly digital computation). This is a partial finding that refines the hypothesis: the mind may perform computations, but we must be cautious in oversimplifying it as just a standard computer – an insight that untangles a misconception in cognitive science. 

 Limitations of Shannon’s Theory without Meaning: The author’s compilation underscores that classical information theory’s neglect of semantics is a critical limitation. In the notes, sources 

### • 1 2 • 3 3 • • 3 3 • 


 point out that applying Shannon’s information theory to fields like biology has been “very disappointing” precisely because the theory ignores meaning. A partial conclusion here is that any measure or theory of information will remain incomplete for living or cognitive systems unless it accounts for meaning/content, not just signal probability. This finding supports the idea that progress in AI or cognitive modeling might require a new information-theoretic approach where the value and context of information are quantified. 

 Cross-disciplinary Evidence: The research notes aggregate ideas from cognitive science, philosophy, and information theory. Although not a finalized conclusion, this cross-pollination itself is a result – it indicates that a synthesis is forming. For instance, the author has brought together Piccinini’s theory-neutral view of computation and Jumarie’s questions on meaningful information, revealing that both disciplines independently recognize a gap (one in defining computation, the other in defining information) that could actually be two sides of the same coin. This convergence is a partial conclusion suggesting that the introduction of semantic concepts might resolve longstanding debates (like classicism vs connectionism, or data vs meaning in information theory) by providing a common framework. 

_(The notes so far stop short of providing that unified framework, but they identify these converging threads, which is a significant intermediate result.)_ 

## Open Questions and Unfinished Threads 

 How to Quantify Meaning? A prominent open question is: How can we introduce and measure meaning within an information-theoretic framework? The notes list this as “of utmost importance.” Concretely, this breaks down into: (i) What is a proper way to inject semantic significance into Shannon’s information theory? and (ii) How could we define or measure the amount of meaningful information in a pattern or message, possibly without relying purely on probability?. These questions remain unanswered in the notes, indicating an unfinished thread where theoretical development or new metrics are needed. 

 What Kind of Computation is Cognition? Another unresolved thread is determining what specific form of computation (if any) best describes cognitive processes. The notes acknowledge that while cognition is some kind of computation, it’s unclear whether it maps onto known models like digital symbol-manipulation, parallel distributed processing, or something entirely different. This question is still open: the author hasn’t concluded if we need new computational paradigms to explain thinking, or if existing paradigms can be tweaked. Answering this likely requires further theoretical work or empirical validation from neuroscience (an area noted but not fully explored in the current dumps). 

 Symbol Grounding and Semantic Emergence: Implicitly, the need to connect meaning with computation raises the classic symbol grounding problem – essentially, how do symbols (or data) gain meaning? While not explicitly solved in the notes, this issue lingers as an open question. The reference to introducing meaning “when we use the model” and examining linguistic notions suggests the author is aware that meaning might emerge from interaction or context. But the exact mechanism of how a computational system could understand a concept (rather than just store or transmit data) is left as an open problem. This thread likely needs more exploration, perhaps drawing on linguistics or cognitive psychology, and is a clear next step in the research. 

### 4 4 • • 1 1 • 3 • 


 Integration of Perspectives: Lastly, an unfinished but evident thread is the integration of the diverse sources and ideas collected. The notes have fragmented pieces – definitions from philosophy, theoretical questions from physics/information theory, and cognitive science debates. An open challenge is weaving these into a coherent hypothesis or model. For example, how does the notion of “generic computation” from cognitive science fit with the quest for a “meaningful information measure” from entropy physics? The answer isn’t in the notes yet, but the pieces to explore it are on the table. Formulating a unified theory or at least a working model that addresses both will be an ongoing task for the author (and collaborators). 

## Opportunities for Refinement and Expansion 

 Theory Synthesis: Given the interdisciplinary findings, a clear opportunity is to synthesize a new framework that combines insights from both cognitive science and information theory. We could, for instance, attempt to extend the computational theory of mind by explicitly incorporating a semantic information metric. By leveraging the noted idea that cognition is “computation in a generic sense” and answering the call to include meaning in information measures , we may develop a meaning-aware computational model of cognition. This would be a significant refinement: turning the current hypotheses into a formal theory that others can critique or test. 

 Empirical Exploration: The notes so far are conceptual, so another opportunity is to translate these ideas into empirical research or simulations. For example, we might design experiments with AI systems (like neural networks or language models) to test how adding a notion of “meaning” changes their performance or outputs. One idea is to measure information content with and without semantic context to see the difference (addressing question (ii) above). Another is to simulate simple cognitive tasks under different computational paradigms (digital vs analog vs neural) to see which best captures properties of human cognition. Such practical forays would expand the work and could either support or refine the theoretical ideas. They would also bring clarity to abstract questions by grounding them in observable outcomes. 

 Clarifying Definitions and Scope: As collaborators, we should also consider tightening the definitions used in this research. Terms like “information”, “meaning”, and “computation” carry different meanings across fields. A short-term refinement could be to clearly define these terms in the context of our project. For instance, distinguishing Shannon information from semantic information , or computational processes vs. algorithmic implementations. The current notes touch on these (e.g. differentiating types of computation, or discussing “смысл/meaning”), but a more formal definition section would prevent confusion as the research deepens. This step would ensure that when we say “meaning” or “information processing,” we have a precise concept that can be measured or modeled, which will make subsequent hypotheses and experiments more rigorous. 

 Addressing Unanswered Questions: The open questions identified are not just gaps – they are opportunities to push the research forward. For the question of introducing meaning into information theory, one expansion could be exploring existing frameworks of “semantic information” (for example, Floridi’s theory of strongly semantic information, or work in semiotics) and seeing how they might plug into our computational view. For the question of what computation cognition is, we might look into recent developments in neuroscience (like integrated information theory, or analog computing in brains) to inform our stance. Each unfinished thread in the notes can be turned into a targeted research question for us to tackle. By prioritizing these – perhaps starting 

### • 

### • 

 3 1 

### • 

### • 

### • 


 with the quantification of meaningful information – we can systematically build on the groundwork laid out on 21JUN25. 

 Interdisciplinary Collaboration: Finally, an opportunity which the notes themselves exemplify is to engage multiple disciplines in this inquiry. The author has drawn from physics (entropy), computer science (information theory, AI), philosophy (mind and meaning), and linguistics. We should continue this integrative approach. For example, collaborating with a linguist could help operationalize the concept of meaning (as hinted by the Russian “смысл” note), while working with a neuroscientist could shed light on how the brain might encode meaning. Such collaborations can provide new insights or data, leading to refinement of our hypotheses. The richness of the prior work dumps suggests that the problem of meaning and computation cannot be solved in one domain alone – which is a challenge but also a chance to create a well-rounded, robust theory with contributions from all sides. 

In summary, the prior research notes from 21JUN25 have surfaced bold hypotheses at the intersection of computation and meaning, gathered evidence that both supports and complicates these ideas, and left us with clear questions to answer. Our next steps should build on this foundation by merging these insights into a coherent framework and pursuing answers through further reading, dialogue, and experimentation. By doing so, we can refine the initial ideas into a mature research direction and possibly break new ground on understanding how **meaningful information processing** underlies cognition and intelligent systems. The opportunity now is to turn these insightful notes into a concrete research plan, leveraging the identified threads to guide a fruitful investigation. 

**Sources:** The analysis above is based on the contents of the repository agladysh/21JUN25-research (notably the files **“We introduce meaning…”** and **“cognition as process of computation”** in the June 21, 2025 inbox), which include excerpts from G. Piccinini’s work on computation and cognition and G. Jumarie’s discussion of extending information theory , among other materials. These sources provide the evidence for the hypotheses, findings, and questions discussed. 

Maximum Entropy, Information Without Probability and Complex Fractals: Classical and Quantum Approach | SpringerLink https://link.springer.com/book/10.1007/978-94-015-9496-7?error=cookies_not_supported&code=91df8dff-649c-401c-9a5deb2bcfce4f4f 

Information processing, computation, and cognition PMC https://pmc.ncbi.nlm.nih.gov/articles/PMC3006465/ 

### • 

 3 1 

 1 2 4 

 3 


